{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT_TEST.ipynb","provenance":[],"mount_file_id":"1iSPkGIANT6z_Nhh_HP_zopSS3e_XMtSC","authorship_tag":"ABX9TyOuiM7PaCsQF2z/AxjMyNLY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"xPniR3NFIPpv"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EqgBiO3oJI2_"},"source":["sentense = pd.read_excel('drive/My Drive/tagging.xlsx', sheet_name='문장f')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"icxSkm9HJPrx"},"source":["paper = pd.read_excel('drive/My Drive/tagging.xlsx', sheet_name='논문f')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TiTYuq22Jj1G"},"source":["sentense = sentense.replace({'\\r' : '', '\\n' : ''}, regex = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lnVTCmqjQLSf"},"source":["sentense = sentense.drop('대표여부', axis = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qzKxh6Y2JplW"},"source":["%tensorflow_version 1.x "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H2BINS22dJAN"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gQYbJZ1fKQUv"},"source":["import tensorflow as tf\n","import torch\n","\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","import time\n","import datetime"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HGXMPazqRzAZ"},"source":["from sklearn.preprocessing import LabelEncoder\n","\n","le = LabelEncoder()\n","result = le.fit_transform(sentense['태그'])\n","le_idx = dict(zip(list(le.classes_), le.transform(list(le.classes_))))\n","print(le_idx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X4D7V75_T5LD"},"source":["sentense['태그'] = result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RIxIhx87PkxP"},"source":["sentense_train, sentense_test = train_test_split(sentense, test_size = 0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u3BcZgt7Rs5A"},"source":["sentense_train.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uCGMfefyP5DP"},"source":["print(sentense_train.shape)\n","print(sentense_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FecemeLeQ9ko"},"source":["train = sentense_train['문장']\n","train[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bbl1FdzRROw_"},"source":["train = [\"[CLS]\" + str(train_sen) + \"[SEP]\" for train_sen in train]\n","train[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8MhUIdHXRiKc"},"source":["label = sentense_train['태그'].values\n","label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SVZuX4lCUH5J"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n","tokenizer_text = [tokenizer.tokenize(sent) for sent in train]\n","\n","print(train[0])\n","print(tokenizer_text[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pcoEc1zUU1Du"},"source":["# 입력 토큰의 최대 시퀀스 길이\n","MAX_LEN = 128\n","\n","# 토큰을 숫자 인덱스로 변환\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenizer_text]\n","\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","input_ids[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u4rwBLPjVlaQ"},"source":["# 어텐션 마스크 초기화\n","attention_masks = []\n","\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)\n","\n","print(attention_masks[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X0sNlq1nVxl0"},"source":["# 훈련셋과 검증셋으로 분리\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, label,random_state=42,test_size=0.1)\n","\n","# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks,input_ids,random_state=42,test_size=0.1)\n","\n","# 데이터를 파이토치의 텐서로 변환\n","train_inputs = torch.tensor(train_inputs)\n","train_labels = torch.tensor(train_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_inputs = torch.tensor(validation_inputs)\n","validation_labels = torch.tensor(validation_labels)\n","validation_masks = torch.tensor(validation_masks)\t\t\t\t\n","\n","print(train_inputs[0])\n","print(train_labels[0])\n","print(train_masks[0])\n","print(validation_inputs[0])\n","print(validation_labels[0])\n","print(validation_masks[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NW5M6M9VXqRz"},"source":["# 배치 사이즈\n","batch_size = 32\n","\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bCP04X9BX0aM"},"source":["test = sentense_test['문장']\n","test[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1gden-Nva5GP"},"source":["test = [\"[CLS]\" + str(test_sen) + \"[SEP]\" for test_sen in test]\n","test[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qg878fi9bNBX"},"source":["label = sentense_test['태그'].values\n","label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RgPcZlaebZ7c"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n","tokenizer_text = [tokenizer.tokenize(sent) for sent in test]\n","\n","print(test[0])\n","print(tokenizer_text[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wJI4eyscbvsE"},"source":["# 입력 토큰의 최대 시퀀스 길이\n","MAX_LEN = 128\n","\n","# 토큰을 숫자 인덱스로 변환\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenizer_text]\n","\n","# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","input_ids[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pbu8MTNycCqk"},"source":["# 어텐션 마스크 초기화\n","attention_masks = []\n","\n","# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)\n","\n","print(attention_masks[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4H-m0ZxPcEhD"},"source":["# 데이터를 파이토치의 텐서로 변환\n","test_inputs = torch.tensor(input_ids)\n","test_labels = torch.tensor(label)\n","test_masks = torch.tensor(attention_masks)\n","\n","print(test_inputs[0])\n","print(test_labels[0])\n","print(test_masks[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m6wOmbNBcGav"},"source":["# 배치 사이즈\n","batch_size = 32\n","\n","# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n","# 학습시 배치 사이즈 만큼 데이터를 가져옴\n","test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_sampler = RandomSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZpABagzzcKxj"},"source":["# GPU 디바이스 이름 구함\n","device_name = tf.test.gpu_device_name()\n","\n","# GPU 디바이스 이름 검사\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ONoEUQPpce_y"},"source":["# 디바이스 설정\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using the CPU instead.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rXUFUcG-diw0"},"source":["# 분류를 위한 BERT 모델 생성\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=9)\n","model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WjX0EBa7d3pD"},"source":["# 옵티마이저 설정\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # 학습률\n","                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n","                )\n","\n","# 에폭수\n","epochs = 4\n","\n","# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n","total_steps = len(train_dataloader) * epochs\n","\n","# 처음에 학습률을 조금씩 변화시키는 스케줄러 생성\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x04_xhiuuQFF","executionInfo":{"status":"ok","timestamp":1605438591130,"user_tz":-540,"elapsed":1070,"user":{"displayName":"JH Kim","photoUrl":"","userId":"06043341777892287476"}}},"source":["# 정확도 계산 함수\n","def flat_accuracy(preds, labels):\n","    \n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"HO1qnOwImMvW","executionInfo":{"status":"ok","timestamp":1605438596825,"user_tz":-540,"elapsed":647,"user":{"displayName":"JH Kim","photoUrl":"","userId":"06043341777892287476"}}},"source":["# 시간 표시 함수\n","def format_time(elapsed):\n","\n","    # 반올림\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # hh:mm:ss으로 형태 변경\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"hjomZbhVmOGB","executionInfo":{"status":"ok","timestamp":1605440945320,"user_tz":-540,"elapsed":2332367,"user":{"displayName":"JH Kim","photoUrl":"","userId":"06043341777892287476"}},"outputId":"c3bb8658-c839-4108-a67c-8a1dadf00e01","colab":{"base_uri":"https://localhost:8080/"}},"source":["# 재현을 위해 랜덤시드 고정\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# 그래디언트 초기화\n","model.zero_grad()\n","\n","# 에폭만큼 반복\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # 시작 시간 설정\n","    t0 = time.time()\n","\n","    # 로스 초기화\n","    total_loss = 0\n","\n","    # 훈련모드로 변경\n","    model.train()\n","        \n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for step, batch in enumerate(train_dataloader):\n","        # 경과 정보 표시\n","        if step % 500 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        # Forward 수행                \n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask, \n","                        labels=b_labels)\n","        \n","        # 로스 구함\n","        loss = outputs[0]\n","\n","        # 총 로스 계산\n","        total_loss += loss.item()\n","\n","        # Backward 수행으로 그래디언트 계산\n","        loss.backward()\n","\n","        # 그래디언트 클리핑\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # 그래디언트를 통해 가중치 파라미터 업데이트\n","        optimizer.step()\n","\n","        # 스케줄러로 학습률 감소\n","        scheduler.step()\n","\n","        # 그래디언트 초기화\n","        model.zero_grad()\n","\n","    # 평균 로스 계산\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    #시작 시간 설정\n","    t0 = time.time()\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 변수 초기화\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for batch in validation_dataloader:\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # 그래디언트 계산 안함\n","        with torch.no_grad():     \n","            # Forward 수행\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # 로스 구함\n","        logits = outputs[0]\n","\n","        # CPU로 데이터 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # 출력 로짓과 라벨을 비교하여 정확도 계산\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"execution_count":37,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch   500  of  1,428.    Elapsed: 0:03:18.\n","  Batch 1,000  of  1,428.    Elapsed: 0:06:35.\n","\n","  Average training loss: 0.86\n","  Training epcoh took: 0:09:24\n","\n","Running Validation...\n","  Accuracy: 0.75\n","  Validation took: 0:00:19\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch   500  of  1,428.    Elapsed: 0:03:18.\n","  Batch 1,000  of  1,428.    Elapsed: 0:06:35.\n","\n","  Average training loss: 0.64\n","  Training epcoh took: 0:09:24\n","\n","Running Validation...\n","  Accuracy: 0.75\n","  Validation took: 0:00:19\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch   500  of  1,428.    Elapsed: 0:03:17.\n","  Batch 1,000  of  1,428.    Elapsed: 0:06:34.\n","\n","  Average training loss: 0.53\n","  Training epcoh took: 0:09:23\n","\n","Running Validation...\n","  Accuracy: 0.76\n","  Validation took: 0:00:19\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch   500  of  1,428.    Elapsed: 0:03:17.\n","  Batch 1,000  of  1,428.    Elapsed: 0:06:34.\n","\n","  Average training loss: 0.44\n","  Training epcoh took: 0:09:23\n","\n","Running Validation...\n","  Accuracy: 0.76\n","  Validation took: 0:00:19\n","\n","Training complete!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RsoY9FlvmSV8","executionInfo":{"status":"ok","timestamp":1605441047114,"user_tz":-540,"elapsed":49470,"user":{"displayName":"JH Kim","photoUrl":"","userId":"06043341777892287476"}},"outputId":"b827ff80-b947-4e54-f7c3-04f447ce93f8","colab":{"base_uri":"https://localhost:8080/"}},"source":["#시작 시간 설정\n","t0 = time.time()\n","\n","# 평가모드로 변경\n","model.eval()\n","\n","# 변수 초기화\n","eval_loss, eval_accuracy = 0, 0\n","nb_eval_steps, nb_eval_examples = 0, 0\n","\n","# 데이터로더에서 배치만큼 반복하여 가져옴\n","for step, batch in enumerate(test_dataloader):\n","    # 경과 정보 표시\n","    if step % 100 == 0 and not step == 0:\n","        elapsed = format_time(time.time() - t0)\n","        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n","\n","    # 배치를 GPU에 넣음\n","    batch = tuple(t.to(device) for t in batch)\n","    \n","    # 배치에서 데이터 추출\n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","    \n","    # 로스 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    \n","    # 출력 로짓과 라벨을 비교하여 정확도 계산\n","    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","    eval_accuracy += tmp_eval_accuracy\n","    nb_eval_steps += 1\n","\n","print(\"\")\n","print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","print(\"Test took: {:}\".format(format_time(time.time() - t0)))"],"execution_count":38,"outputs":[{"output_type":"stream","text":["  Batch   100  of    397.    Elapsed: 0:00:12.\n","  Batch   200  of    397.    Elapsed: 0:00:24.\n","  Batch   300  of    397.    Elapsed: 0:00:37.\n","\n","Accuracy: 0.76\n","Test took: 0:00:48\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KFe6pqsAvYh8","executionInfo":{"status":"ok","timestamp":1605441080892,"user_tz":-540,"elapsed":1110,"user":{"displayName":"JH Kim","photoUrl":"","userId":"06043341777892287476"}}},"source":["# 입력 데이터 변환\n","def convert_input_data(sentences):\n","\n","    # BERT의 토크나이저로 문장을 토큰으로 분리\n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","\n","    # 입력 토큰의 최대 시퀀스 길이\n","    MAX_LEN = 128\n","\n","    # 토큰을 숫자 인덱스로 변환\n","    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","    \n","    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n","    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","    # 어텐션 마스크 초기화\n","    attention_masks = []\n","\n","    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n","    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","\n","    # 데이터를 파이토치의 텐서로 변환\n","    inputs = torch.tensor(input_ids)\n","    masks = torch.tensor(attention_masks)\n","\n","    return inputs, masks"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"Me4Ya7YwvseK","executionInfo":{"status":"ok","timestamp":1605441089571,"user_tz":-540,"elapsed":1418,"user":{"displayName":"JH Kim","photoUrl":"","userId":"06043341777892287476"}}},"source":["# 문장 테스트\n","def test_sentences(sentences):\n","\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 문장을 입력 데이터로 변환\n","    inputs, masks = convert_input_data(sentences)\n","\n","    # 데이터를 GPU에 넣음\n","    b_input_ids = inputs.to(device)\n","    b_input_mask = masks.to(device)\n","            \n","    # 그래디언트 계산 안함\n","    with torch.no_grad():     \n","        # Forward 수행\n","        outputs = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","\n","    # 로스 구함\n","    logits = outputs[0]\n","\n","    # CPU로 데이터 이동\n","    logits = logits.detach().cpu().numpy()\n","\n","    return logits"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"txQwTlG8vuoy","executionInfo":{"status":"ok","timestamp":1605441343156,"user_tz":-540,"elapsed":968,"user":{"displayName":"JH Kim","photoUrl":"","userId":"06043341777892287476"}},"outputId":"befb9f21-7074-4e42-d205-d7590226cc78","colab":{"base_uri":"https://localhost:8080/"}},"source":["logits = test_sentences(['향후 연구에서는 본 연구의 결과를 기반으로 클라우드 환경에서의 현재 운용되고 있는 클라우드 서비스에 사용자 인증 모델을 적용할 계획이다'])\n","\n","print(logits)\n","# le_idx[np.argmax(logits)]\n","le_idx"],"execution_count":48,"outputs":[{"output_type":"stream","text":["[[-0.38870597 -0.5936224  -1.3362468  -1.5381625  -0.3343552   1.056934\n","  -1.5686027  -1.2347175   6.77139   ]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'가설 설정': 0,\n"," '기술 정의': 1,\n"," '대상 데이터': 2,\n"," '데이터처리': 3,\n"," '문제 정의': 4,\n"," '성능/효과': 5,\n"," '이론/모형': 6,\n"," '제안 방법': 7,\n"," '후속연구': 8}"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"n25NiLjHvv1I","executionInfo":{"status":"ok","timestamp":1605441470640,"user_tz":-540,"elapsed":999,"user":{"displayName":"JH Kim","photoUrl":"","userId":"06043341777892287476"}},"outputId":"d7543453-cdcb-45b2-af09-69928813d707","colab":{"base_uri":"https://localhost:8080/"}},"source":["logits = test_sentences(['본 논문에서는 클라우드 환경에서 사용자의 인증 효율성을 개선시키기 위해서 인증서버와 클라우드 관리자 사이에서 사용자의 접근 권한을 제공하기 위한 2중 키 기반의 사용자 인증 모델을 제안하였다.'])\n","\n","print(logits)\n","print(np.argmax(logits))"],"execution_count":50,"outputs":[{"output_type":"stream","text":["[[-2.314767   -2.3063498  -1.8921778  -0.6616681   4.1249547   0.4490954\n","  -0.94905823  4.190033   -1.037311  ]]\n","7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Wl-C9v_zxGuI"},"source":[""],"execution_count":null,"outputs":[]}]}